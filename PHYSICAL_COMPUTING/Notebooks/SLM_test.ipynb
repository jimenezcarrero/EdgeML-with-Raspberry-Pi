{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63f138d2-2618-4159-b954-16e91eb95f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e952af05-bed1-4b19-af64-35c0449e1e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available (it won't be on our case, Raspberry Pi)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f6319d-1a08-4886-b023-a9a2e24fb094",
   "metadata": {},
   "source": [
    "## TinyLlama-1.1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "669e8dd1-05b1-4b53-aff0-ff8c97800058",
   "metadata": {},
   "outputs": [],
   "source": [
    " model='TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6f9d44b-103e-475e-b2a0-33d73db84b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loading time: 1.28 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load the model and measure loading time\n",
    "start_time = time.time()\n",
    "generator = pipeline('text-generation', \n",
    "                    model=model,\n",
    "                    device=device)\n",
    "load_time = time.time() - start_time\n",
    "print(f\"Model loading time: {load_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdf067f4-e93f-4f34-8825-5074c9ef214d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/mjrovai/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test prompt: The weather today is\n",
      "Generated response: The weather today is going to be sunny and warm with a high of 80 degrees.\n",
      "The weather today is going to be sunny and warm with a high of 80 degrees.\n",
      "The weather today is going to be\n",
      "Inference time: 199.41 seconds\n"
     ]
    }
   ],
   "source": [
    "# Test prompt\n",
    "test_prompt = \"The weather today is\"\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "response = generator(test_prompt, \n",
    "                    max_length=50,\n",
    "                    num_return_sequences=1,\n",
    "                    temperature=0.7)\n",
    "inference_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTest prompt: {test_prompt}\")\n",
    "print(f\"Generated response: {response[0]['generated_text']}\")\n",
    "print(f\"Inference time: {inference_time:.2f} seconds\")\n",
    "\n",
    "# Memory usage\n",
    "if device == \"cuda\":\n",
    "    print(f\"\\nGPU Memory allocated: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
    "    print(f\"GPU Memory cached: {torch.cuda.memory_reserved()/1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73e2306-596d-4033-9204-6e2f279dc476",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
